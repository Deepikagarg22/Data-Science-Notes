{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "linear regression\n",
    "support vector regression\n",
    "decision tree regression\n",
    "random forest reg\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression\n",
    "support vector class\n",
    "decision tree class\n",
    "random forest\n",
    "baive bayes "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agaboost\n",
    "xgaboost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Data  Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.Data Cleaning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "either remove that row who have missing value (when data is large)\n",
    "\n",
    "use this when data is max and minimum missing value  (df = df.dropna())\n",
    "\n",
    "either fill \n",
    "\n",
    "if there is stock data then  df = df.fillna(method=\"ffill\")  for filling previous"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.Dependent and independent variable**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = df.iloc[:,:-1]\n",
    "\n",
    "y = df['charges']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.Feature Engineering** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " deleting irrelevant columns "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4.Encoding categorical column**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for categorical \n",
    "\n",
    "**ordinal variable** = ordinal variable is that in which categories have ordinal/sequential relationship between them and that relationship effect the dependent variable.\n",
    "\n",
    "**not ordinal** = categories relation does not effect dependent variable but individually may effect\n",
    "\n",
    "**FOR EXAMPLE** = ordinal means ek dusre se bde ho ,  quality btaye , medium small (category h ek dusre se bde hone ka meaning btare h) , sequential relationship \n",
    "\n",
    "not ordianal = jo ek dusre   se bde hona na btare ho (like south , north , east, west)\n",
    "\n",
    "**Ordinal = Label encoding**\n",
    "\n",
    "**not ordinal = onehot encoding**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dummy variable trap**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when one variable is highly correlated with another variable so rest of variables start predicting that variable\n",
    "\n",
    "means hm ek delete krde dummy variable "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sparsed array**\n",
    "\n",
    "when there is maximum zero values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5.Feature Selection**\n",
    "\n",
    "process of choosing inportant columns (features,columns,variables,dimentions)\n",
    "\n",
    "**case 1** = independent numeric , dep numeric  ----------------------            -- pearson correlation matrix\n",
    "\n",
    "**case 2** = cat indepen , dep num      --------------------------------                 -- ANOVA test (analysis of variance)\n",
    "\n",
    "**Case 3** = indep cat , dep cat      -------------------------------                   --  chi square test , mutual information\n",
    " \n",
    " **Case 4** = ind numeric , cat dep     -----------------------------------                  -- ANOVA test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ANOVA test** \n",
    "it tells us weather the variance of each category is same or different \n",
    "category ki variance check krte h fir , variance difference hua to important column h otherwise column unimportant h\n",
    "\n",
    "terms\n",
    "\n",
    "- within sum of square = sum of square of average value of each group to the grand mean \n",
    "- between sum of square = sum of squre of value of each group to the mean of that group\n",
    "\n",
    "wss/dof  /  bss/dof1\n",
    "\n",
    "dof = no. of categories - 1\n",
    "\n",
    "degree of freedom 1 = no. of sample - 1\n",
    "\n",
    "dof simple means kitni values ko change kr skte h\n",
    "\n",
    "**Code**\n",
    "\n",
    "- from sklearn.feature_selection import SelectKBest,f_classif\n",
    "- kbest = SelectKBest(f_classif,k=categorical.shape[1])\n",
    "- kbest.fit_transform(categorical,numeric['selling_price'])\n",
    "- print(\"F values\\n\",kbest.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Outlier Detection**\n",
    "two methods \n",
    "- follow normal dist    - z-square method \n",
    "- follow non normal dist - dbscan method (density based spatial clusturing algorithm)\n",
    "\n",
    "**z_score** and **Standard Deviation Method** = x - mean / sd -----2.4 point lie in second standard deviation ,z_score tells the data lie in which standard deviation ,we apply z_score when data is in normal distribution\n",
    "\n",
    "- from scipy import stats\n",
    "- import numpy as np\n",
    "- z = np.abs(stats.zscore(boston_df))\n",
    "- print(z)\n",
    "- threshold = 3\n",
    "- print(np.where(z > 3))\n",
    "\n",
    "**Interquartile Range or Percentage Method** = when the data is not in normal distribution \n",
    "\n",
    "**DBSCAN**\n",
    "\n",
    "made for clustering\n",
    "\n",
    "2 paramters:\n",
    "\n",
    "1. e(eps):radius of circle around each point.\n",
    "2. minimum points: minimum num of samples to be inside circle to have a core point.\n",
    "\n",
    "\n",
    "Core point: having minimum number points inside circle.\n",
    "\n",
    "\n",
    "border point: in which number of sample less than minimum point.\n",
    "\n",
    "\n",
    "noise: in whch there is no sample inside a circle.\n",
    "\n",
    "\n",
    "\n",
    "jis circle ke andhr koi bhi point nhi aa rha voh outlirs he\n",
    "\n",
    "\n",
    "\n",
    "**requirement:**\n",
    "data should be scaled\n",
    "\n",
    "how to choose eps value\n",
    "\n",
    "\n",
    "avg value of distance of a sample to its nearest neigbour.\n",
    "\n",
    "to find that use kdistance graph.(ik ik point ka nearest neighbour ka distance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6.Scaling**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**min max scaler** = x-x min  / x max - x min  (maxmin can given by own also)  (distribution nhi pta , data nhi pta)\n",
    "\n",
    "**standard scaler** = x - x mean  /  standard deviation   (normal distribution pta h , data k bare m full pta h jaise car,bmi )\n",
    "\n",
    "**Data Distribution** = column m jo value h ek ek value kitni bar aa rhi h 46\n",
    "\n",
    "jo natural data hota h wo normally distributed hota h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Splitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random state = 0 when we don't want rondom numbers\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "\n",
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Prediction / Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**regression metrices**\n",
    "\n",
    "mean squared error\n",
    "\n",
    "mean absolute error  = sigma( y_pred - y_test )/no. of \n",
    "\n",
    "root mean squared error\n",
    "\n",
    "**r2_score** = tells us how much variance of dependent variable is explained by independent variables (most good method )\n",
    "- 1 = no error\n",
    "- 0.8 se upr  = good / normal \n",
    "- - m h = very bad\n",
    "\n",
    "**classification metrices**\n",
    "\n",
    "accuracy_score\n",
    "\n",
    "precision\n",
    "\n",
    "recall\n",
    "\n",
    "confusion matrix\n",
    "\n",
    "ROC curve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Support_Vector_Machine**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minimum width ho or sbhi point se distance km ho then it is plane \n",
    "\n",
    "hyperparameter tuning = change the parameter of modet that effects learning capability \n",
    "\n",
    "when accuracy is not good in classification the we use **SVC** \n",
    "\n",
    "**C-PARAMETER** is regularisation parameter     **(tuning)**\n",
    "\n",
    "it is very slow also (outliers ko ignore krti h ye jo pass ho , kuch ko hi deal krti h)\n",
    "\n",
    "kernal = rbf,poly,sigmoid,linear(normal distribution m rbf lgta h)\n",
    "\n",
    "one dimention m plane dhundna = dot\n",
    "\n",
    "two d m dhundna = line \n",
    "\n",
    "3 d m dhundna = box"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree Algorithm**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrophy** = measure of uncertainty ,  e = sum of (-pi (log pi)) , i = class\n",
    "  \n",
    "pure data ko ek node pr rkhenge then find entrophy\n",
    "split krdenge fir dekhenge kon si splitting m sbse km entropy AARI h or us column ko choose kr lenge\n",
    "\n",
    "check krenge ki koi aisi class h jisme pure class h us node ko krdenge bnd \n",
    "\n",
    "E(playgolf,oulook ) = p(sunny)*E(3,2) + p(overcast)*E(4,0) + P(Rainy)*E(2,3)         (gini)\n",
    "\n",
    "**criterion** = gini , entrophy,log_loss\n",
    "\n",
    "**splitter** = best , random (default=best)\n",
    "\n",
    "**max_depth** is the no. of levels in decision tree , regularisation control hoti h , 10 rkhi h to 10 level hi bnege\n",
    "\n",
    "**MIN_SAMPLE_SPLIT** = minimum no. of samples required to split an internal node.\n",
    "\n",
    "**max_feature** = kitne column dene h\n",
    "\n",
    "**random_state** = change na ho , shuffling na ho\n",
    "\n",
    "**Underfitting vs Overfitting** = underfitting is when model fails to learn the data and overfitting is model try to learn each and every point.\n",
    "\n",
    "underfitting m low variance and high bias\n",
    "\n",
    "overfitting = high variance low bias\n",
    "\n",
    "bias = assumptions leta h\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Standard Deviation** \n",
    "\n",
    " Data ek dusre se kitna door h, jb sd jyada ho then fir se splitting krte h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TYPE ERROR**\n",
    "glt type p operation kre agr , str +int\n",
    "\n",
    "# **Value Error**\n",
    "type bi shi h sb shi h but function p value glt dedi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Normal Distribution**\n",
    "\n",
    "koi bi column normal distribution m h to aage predictions lgani easy hoti h\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Skewness**\n",
    "\n",
    "column ka graph normal distribution se kitna alg h (value = -0.5 to 0.5 then purely normal distributed) (-1 to 0.5 then negative )\n",
    "\n",
    "how much our data vary from normal dist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Kurtosis**\n",
    "\n",
    "tells the peakedness/ flatness of data , use = stock market  m jb curve platy kurtic ho to risk is more , jb meso kutic ho to risk is more or less"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Variance**\n",
    "\n",
    "ek columns kitni judi hui h aaps m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
